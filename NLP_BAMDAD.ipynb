{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMm4qWjEu7rcIstjnMvlFUv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BamdadBooyeh/BamdadBooyeh/blob/main/NLP_BAMDAD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7zzo1mLATYQ",
        "outputId": "d4dc6e42-b2ed-4ecc-9f51-e61e7d284202"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lime in /usr/local/lib/python3.11/dist-packages (0.2.0.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from lime) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from lime) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lime) (1.15.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from lime) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.11/dist-packages (from lime) (1.6.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.11/dist-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (11.2.1)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (2025.6.11)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (24.2)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.18->lime) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->lime) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n",
            "Requirement already satisfied: captum in /usr/local/lib/python3.11/dist-packages (0.8.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from captum) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0 in /usr/local/lib/python3.11/dist-packages (from captum) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from captum) (24.2)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.11/dist-packages (from captum) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from captum) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10->captum) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10->captum) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->captum) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.10->captum) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Install the lime library\n",
        "!pip install lime\n",
        "!pip install captum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dyuw0s6gs7R_"
      },
      "source": [
        "# Data preprocessing and loader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "\n",
        "In this project, we apply core concepts from the course to a Natural Language Processing (NLP) task: emotion classification of English Twitter messages. We use the Emotion dataset from HuggingFace Datasets (dair-ai/emotion), which consists of short texts labeled with one of six emotion classes:\n",
        "\n",
        "\t•\t0: sadness\n",
        "\t•\t1: joy\n",
        "\t•\t2: love\n",
        "\t•\t3: anger\n",
        "\t•\t4: fear\n",
        "\t•\t5: surprise\n",
        "\n",
        "The dataset comes with predefined train, validation, and test splits.\n",
        "\n",
        "The goal is to build a pipeline that prepares this text data and train deep learning models to classify text emotion.\n",
        "\n",
        "# Data Preprocessing and DataLoader Construction\n",
        "\n",
        "1. **Dataset Loading**\n",
        "\n",
        "The dataset splits (train, validation, test) were loaded from Hugging Face using the .parquet format and converted into pandas DataFrames for easier manipulation.\n",
        "\n",
        "2. **Label Distribution Analysis**\n",
        "\n",
        "We analyzed label counts in each split to assess class imbalance. The training set was heavily imbalanced, with class 1 (“joy”) dominating at ~34%, while class 5 (“surprise”) made up less than 5%. This information is crucial for interpreting model performance.\n",
        "\n",
        "Split\tMost Frequent Label\tAccuracy if Always Predicts\n",
        "Train\tJoy (1)\t34%\n",
        "Chance (random)\t-\t17%\n",
        "\n",
        "3. **Tokenization**\n",
        "\n",
        "A basic whitespace tokenizer was implemented to lowercase and split the text into tokens. This approach was chosen for simplicity and transparency, giving us full control over vocabulary and encoding.\n",
        "\n",
        "4. **Vocabulary Building**\n",
        "\n",
        "We constructed a vocabulary from the training tokens, including special tokens <PAD> and <OOV>, resulting in 15,214 unique tokens. Words unseen in training default to <OOV> during encoding.\n",
        "\n",
        "5. **Text Encoding & Padding**\n",
        "\n",
        "Each sentence was encoded into integer token IDs using our vocabulary, and then padded to a fixed maximum length based on the 95th percentile of training sentence lengths (MAX_LEN = ~50). This ensures consistency across batches and improves training efficiency.\n",
        "\n",
        "6. **Tensor Conversion and DataLoader Creation**\n",
        "\n",
        "Finally, all processed inputs and labels were converted into PyTorch tensors and organized into TensorDataset objects. We wrapped them with DataLoaders using a batch size of 32, with shuffling applied only to the training set.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Key Statistics and Insights\n",
        "\n",
        "\t•\tText Lengths: Mean length across splits was ~19 tokens with a wide spread, justifying our choice of max padding length.\n",
        "\t•\tClass Distribution: Visual plots and counts exposed a high class imbalance, guiding our evaluation strategy.\n",
        "\t•\tVocabulary Size: A large vocabulary (15k+ tokens) suggests diverse language usage, which can help the model generalize well.\n",
        "\n",
        "\t•\tCustom Control: Building our own tokenizer and vocabulary gave us insight and flexibility that libraries like Tokenizer or BERT would abstract away.\n",
        "\t•\tRobustness: Handling edge cases (e.g., short texts, rare tokens) ensured that the model training step would not fail or skew.\n",
        "\t•\tBaseline Understanding: Knowing the majority class accuracy (34%) provides a realistic baseline for evaluating our classifier.\n",
        "\n"
      ],
      "metadata": {
        "id": "aF-ouZmIXxag"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHrbrRJTsoMw"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader # Import DataLoader and TensorDataset here\n",
        "\n",
        "splits = {'train': 'split/train-00000-of-00001.parquet', 'validation': 'split/validation-00000-of-00001.parquet', 'test': 'split/test-00000-of-00001.parquet'}\n",
        "dataset = {\n",
        "    split: pd.read_parquet(f\"hf://datasets/dair-ai/emotion/{path}\")\n",
        "    for split, path in splits.items()\n",
        "}\n",
        "label_names = sorted(dataset[\"train\"][\"label\"].unique())\n",
        "label_names = [str(lbl) for lbl in label_names]  # In case labels are ints\n",
        "\n",
        "def label_stats(split):\n",
        "    # Ensure dataset[split] is a DataFrame here\n",
        "    if not isinstance(dataset[split], pd.DataFrame):\n",
        "        print(f\"Warning: dataset['{split}'] is not a DataFrame when calling label_stats.\")\n",
        "        # You might need to handle this case if label_stats is called after tensor conversion\n",
        "        # For this fix, we assume label_stats is called before the conversion.\n",
        "        return [], Counter() # Return empty results if not a DataFrame\n",
        "\n",
        "    labels = dataset[split][\"label\"].tolist()\n",
        "    counts = Counter(labels)\n",
        "    print(f\"\\n[{split.upper()}] Label Counts:\")\n",
        "    for label, count in sorted(counts.items()):\n",
        "        # Ensure label is within the bounds of label_names\n",
        "        if label < len(label_names):\n",
        "            print(f\"{label_names[label]:>10}: {count}\")\n",
        "        else:\n",
        "            print(f\"Unknown label ({label}): {count}\")\n",
        "    return labels, counts\n",
        "\n",
        "# Re-define splits as a list after the dictionary definition\n",
        "splits = ['train', 'validation', 'test']\n",
        "split_labels = {}\n",
        "for split in splits:\n",
        "    split_labels[split], counts = label_stats(split)\n",
        "\n",
        "# Plot distribution\n",
        "for split in splits:\n",
        "    plt.figure()\n",
        "    labels = split_labels[split]\n",
        "    counts = Counter(labels)\n",
        "    # Use label_names as x-axis labels and counts corresponding to label_names order\n",
        "    plt.bar(label_names, [counts.get(i, 0) for i in range(len(label_names))]) # Use .get(i, 0) to handle potential missing labels\n",
        "    plt.title(f\"{split.capitalize()} label distribution\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Chance accuracy (random guess)\n",
        "total_train = len(split_labels['train'])\n",
        "most_common_class = Counter(split_labels['train']).most_common(1)[0]\n",
        "chance_accuracy = 1 / len(label_names)\n",
        "majority_accuracy = most_common_class[1] / total_train\n",
        "print(f\"\\nChance accuracy: {chance_accuracy:.2f}\")\n",
        "# Ensure index is within bounds\n",
        "if most_common_class[0] < len(label_names):\n",
        "    print(f\"Majority class baseline accuracy: {majority_accuracy:.2f} ({label_names[most_common_class[0]]})\")\n",
        "else:\n",
        "     print(f\"Majority class baseline accuracy: {majority_accuracy:.2f} (Unknown label)\")\n",
        "\n",
        "\n",
        "# STEP 2: Tokenization\n",
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "# Apply tokenization while dataset is still a DataFrame\n",
        "for split in splits:\n",
        "    # Ensure dataset[split] is a DataFrame before applying .apply()\n",
        "    if isinstance(dataset[split], pd.DataFrame):\n",
        "        dataset[split][\"tokens\"] = dataset[split][\"text\"].apply(tokenize)\n",
        "    else:\n",
        "        print(f\"Warning: dataset['{split}'] is not a DataFrame during tokenization.\")\n",
        "\n",
        "# Text length stats\n",
        "# Calculate lengths while dataset[split] is still a DataFrame with a 'tokens' column\n",
        "lengths = {split: [len(ex['tokens']) for _, ex in dataset[split].iterrows()] for split in splits if isinstance(dataset[split], pd.DataFrame)} # Iterate over rows if it's a DataFrame\n",
        "for split in splits:\n",
        "    if split in lengths: # Check if length data was collected for this split\n",
        "        lens = lengths[split]\n",
        "        print(f\"\\n{split.capitalize()} text length - Min: {min(lens)}, Max: {max(lens)}, Mean: {np.mean(lens):.2f}, Std: {np.std(lens):.2f}\")\n",
        "    else:\n",
        "        print(f\"\\nCould not calculate text length stats for {split.capitalize()} (not a DataFrame).\")\n",
        "\n",
        "\n",
        "# STEP 3: Build vocabulary\n",
        "PAD_TOKEN = \"<PAD>\"\n",
        "OOV_TOKEN = \"<OOV>\"\n",
        "\n",
        "# Flatten all tokens in train split\n",
        "# Ensure dataset['train'] is a DataFrame before accessing 'tokens'\n",
        "all_tokens = []\n",
        "if isinstance(dataset['train'], pd.DataFrame):\n",
        "    all_tokens = [token for example in dataset['train']['tokens'] for token in example]\n",
        "else:\n",
        "    print(\"\\nWarning: dataset['train'] is not a DataFrame. Cannot build vocabulary from tokens.\")\n",
        "\n",
        "vocab = {PAD_TOKEN: 0, OOV_TOKEN: 1}\n",
        "if all_tokens: # Only build vocab if tokens were collected\n",
        "    for token in set(all_tokens):\n",
        "        vocab[token] = len(vocab)\n",
        "\n",
        "# Reverse vocab\n",
        "id2word = {idx: word for word, idx in vocab.items()}\n",
        "\n",
        "print(f\"\\nVocabulary size: {len(vocab)}\")\n",
        "\n",
        "# STEP 4: Encode texts with OOV handling\n",
        "def encode_tokens(tokens, vocab):\n",
        "    return [vocab.get(token, vocab[OOV_TOKEN]) for token in tokens]\n",
        "\n",
        "for split in splits:\n",
        "    # Ensure dataset[split] is a DataFrame before applying .apply()\n",
        "    if isinstance(dataset[split], pd.DataFrame):\n",
        "        dataset[split][\"input_ids\"] = dataset[split][\"tokens\"].apply(lambda tokens: encode_tokens(tokens, vocab))\n",
        "    else:\n",
        "         print(f\"Warning: dataset['{split}'] is not a DataFrame during encoding.\")\n",
        "\n",
        "\n",
        "# STEP 5: Padding and tensor conversion\n",
        "# Use MAX_LEN calculated from the 'lengths' dictionary\n",
        "# Check if 'lengths' was successfully populated\n",
        "if 'train' in lengths:\n",
        "    MAX_LEN = int(np.percentile(lengths['train'], 95))\n",
        "else:\n",
        "    MAX_LEN = 50 # Fallback value if lengths could not be calculated\n",
        "    print(f\"Warning: Could not determine MAX_LEN from lengths. Using default: {MAX_LEN}\")\n",
        "\n",
        "\n",
        "def pad_sequence(seq, max_len, pad_val):\n",
        "    # Ensure seq is a list\n",
        "    if not isinstance(seq, list):\n",
        "        print(f\"Warning: pad_sequence received non-list input: {type(seq)}\")\n",
        "        return [] # Return empty list if input is not list\n",
        "    return seq[:max_len] + [pad_val] * max(0, max_len - len(seq))\n",
        "\n",
        "# This is where the dataset is converted from DataFrame to dictionary of tensors\n",
        "for split in splits:\n",
        "    inputs = []\n",
        "    labels = []\n",
        "    # Ensure dataset[split] is a DataFrame with necessary columns before iteration\n",
        "    if isinstance(dataset[split], pd.DataFrame) and 'input_ids' in dataset[split].columns and 'label' in dataset[split].columns:\n",
        "        for index, example in dataset[split].iterrows(): # Iterate over DataFrame rows\n",
        "            # Ensure 'input_ids' is a list before padding\n",
        "            input_ids_list = example['input_ids']\n",
        "            if not isinstance(input_ids_list, list):\n",
        "                 print(f\"Warning: input_ids for example in {split} is not a list: {type(input_ids_list)}\")\n",
        "                 input_ids_list = [] # Use empty list if not a list\n",
        "\n",
        "            padded = pad_sequence(input_ids_list, MAX_LEN, vocab.get(PAD_TOKEN, -1)) # Use .get() for PAD_TOKEN to be safe, fallback to -1\n",
        "            inputs.append(padded)\n",
        "            labels.append(example['label'])\n",
        "\n",
        "        inputs_tensor = torch.tensor(inputs)\n",
        "        labels_tensor = torch.tensor(labels)\n",
        "        dataset[split] = {'inputs': inputs_tensor, 'labels': labels_tensor}\n",
        "    else:\n",
        "        print(f\"Warning: dataset['{split}'] is not a DataFrame with 'input_ids' and 'label' columns. Skipping tensor conversion for this split.\")\n",
        "        # If conversion failed, ensure dataset[split] is not expected to be tensors later\n",
        "        # Depending on the rest of the pipeline, you might want to remove this split from 'loaders' later\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "loaders = {}\n",
        "for split in splits:\n",
        "    # Check if dataset[split] has the expected structure (dictionary with 'inputs' and 'labels' tensors)\n",
        "    if isinstance(dataset[split], dict) and 'inputs' in dataset[split] and 'labels' in dataset[split] and isinstance(dataset[split]['inputs'], torch.Tensor) and isinstance(dataset[split]['labels'], torch.Tensor):\n",
        "        tensor_dataset = TensorDataset(dataset[split]['inputs'], dataset[split]['labels'])\n",
        "        shuffle = True if split == 'train' else False\n",
        "        loaders[split] = DataLoader(tensor_dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "    else:\n",
        "        print(f\"Warning: Could not create DataLoader for {split}. Dataset structure not as expected.\")\n",
        "\n",
        "\n",
        "print(\"\\n Data preparation complete. Loaders are ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuVfUOTztAQv"
      },
      "source": [
        "# First model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We trained a simple feedforward neural network (MLP) for classifying emotions in tweets using the Emotion dataset .\n",
        "\n",
        "## Model Overview\n",
        "\n",
        "\t•\tArchitecture: Embedding → Pooling (avg or max) → Dropout → Linear (MLP-style)\n",
        "\n",
        "\t•\tType: Shallow MLP with pooled embeddings\n",
        "\n",
        "# Training Setup\n",
        "\n",
        "\t•\tOptimizer: Adam, LR = 1e-3\n",
        "\t•\tLoss: CrossEntropy\n",
        "\t•\tEpochs: 50\n",
        "\t•\tDevice: CPU/GPU (based on availability)\n",
        "\n",
        "# Results\n",
        "\n",
        "\t•\tBest Val Accuracy: 88.35% (Epoch 28)\n",
        "\t•\tFinal Test Accuracy: 83.65%\n",
        "\t•\tTrend: Overfitting begins after ~30 epochs (train loss ↓, val loss ↑)\n",
        "\n",
        "# Conclusion\n",
        "\n",
        "This MLP-based model is simple yet effective for emotion classification, but starts overfitting in later epochs.\n",
        "\n",
        " It’s a strong baseline to compare with more advanced models like LSTMs or Transformers.\n"
      ],
      "metadata": {
        "id": "R7zj3YdGZoLR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0j3VN2wntE3H"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class EmotionClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_classes, pad_idx, pool_type='avg', dropout_rate=0.0):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
        "        self.pool_type = pool_type\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)\n",
        "        if self.pool_type == 'max':\n",
        "            pooled, _ = embedded.max(dim=1)\n",
        "        else:  # default to average\n",
        "            pooled = embedded.mean(dim=1)\n",
        "        pooled = self.dropout(pooled)\n",
        "        return self.fc(pooled)\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train_model(model, loaders, num_epochs=20, lr=1e-3, device='cpu'):\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for inputs, labels in loaders['train']:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(loaders['train'])\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in loaders['validation']:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                correct += (preds == labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        avg_val_loss = val_loss / len(loaders['validation'])\n",
        "        accuracy = correct / total\n",
        "        val_losses.append(avg_val_loss)\n",
        "        val_accuracies.append(accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {accuracy:.4f}\")\n",
        "\n",
        "    return train_losses, val_losses, val_accuracies\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_losses(train_losses, val_losses):\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Training vs Validation Loss\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def evaluate(model, loader, device='cpu'):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "# Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 100\n",
        "num_classes = len(label_names)\n",
        "pad_idx = vocab[PAD_TOKEN]\n",
        "\n",
        "# Init + Train\n",
        "model1 = EmotionClassifier(vocab_size, embed_dim, num_classes, pad_idx)\n",
        "train_losses, val_losses, val_accuracies = train_model(model1, loaders, num_epochs=50, lr=1e-3, device=device)\n",
        "\n",
        "# Plot\n",
        "plot_losses(train_losses, val_losses)\n",
        "\n",
        "# Test accuracy\n",
        "test_acc = evaluate(model1, loaders['test'], device=device)\n",
        "print(f\"\\n Final Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inlZ_iNdeJrL"
      },
      "source": [
        "## First model's interpretability"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "To understand how our MLP emotion classifier makes decisions, we used both intrinsic and post-hoc interpretability methods:\n",
        "\n",
        "##\t Intrinsic:\n",
        "print_top_words() identifies the top words most associated with each emotion class based on the classifier’s learned weights.\n",
        "\n",
        "The model highlights words most influential for each emotion\n",
        "\n",
        "\t•\tClass 0 (e.g., sadness): gloomy, deprived, burdened…\n",
        "\t•\tClass 1 (e.g., joy): ecstatic, intelligent, successful…\n",
        "\t•\tClass 2 (e.g., love): fond, compassionate, nostalgic…\n",
        "\t•\tClass 3 (e.g., anger): pissed, cranky, grouchy…\n",
        "\t•\tClass 4 (e.g., fear): frantic, hesitant, paranoid…\n",
        "\t•\tClass 5 (e.g., surprise): amazed, stunned, overwhelmed…\n",
        "\n",
        "##\t Interactive Testing:\n",
        "\n",
        "classify_interactively() allows live predictions and prints class probabilities for user input text.\n",
        "\n",
        "\t•\tInput: “you are so mean!”\n",
        "\t•\tPredicted Emotion: Class 0 (e.g., sadness) with 57% confidence\n",
        "\t•\tShows full class probability breakdown.\n",
        "  \n",
        "##\t LIME (Post-hoc):\n",
        "\n",
        "explain_with_lime() highlights which words influenced the prediction the most for a given sentence using the LIME framework.\n",
        "Input: “I feel miserable and alone”\n",
        "\n",
        "\t•\tLIME highlights “miserable” and “alone” as most impactful.\n",
        "\t•\tThese words are strongly associated with class 0.\n",
        "\t•\tExplains how individual tokens contribute to prediction.\n",
        "\n",
        "These methods help us peek inside the black box and make the model’s behavior more transparent.\n",
        "\n"
      ],
      "metadata": {
        "id": "_ynMhs_HbX2j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h_PLSofwdr94"
      },
      "outputs": [],
      "source": [
        "# Interpretability\n",
        "import torch\n",
        "import numpy as np\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "# PREPROCESS FUNCTION FOR LIME & INTERACTIVE\n",
        "def preprocess_fn(text, max_len=MAX_LEN):\n",
        "    tokens = tokenize(text)\n",
        "    token_ids = [vocab.get(token, vocab[OOV_TOKEN]) for token in tokens]\n",
        "    token_ids = token_ids[:max_len] + [vocab[PAD_TOKEN]] * max(0, max_len - len(token_ids))\n",
        "    return torch.tensor(token_ids, dtype=torch.long)\n",
        "\n",
        "# INTERACTIVE CLASSIFICATION\n",
        "def classify_interactively(model, label_map):\n",
        "    print(\"\\nType a sentence to classify its emotion. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        text = input(\"Enter text: \")\n",
        "        if text.lower() == 'exit':\n",
        "            break\n",
        "        tokens = preprocess_fn(text).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = model(tokens)\n",
        "            probs = torch.softmax(logits, dim=1).squeeze()\n",
        "            pred = torch.argmax(probs).item()\n",
        "        print(f\"Prediction: {label_map[pred]} ({probs[pred]:.2f})\")\n",
        "        print(\"Top class probabilities:\")\n",
        "        for i, p in enumerate(probs):\n",
        "            print(f\"  {label_map[i]}: {p:.2f}\")\n",
        "\n",
        "# CLASS-WORD ASSOCIATION (INTRINSIC)\n",
        "def print_top_words(model, id2word, class_names, top_n=10):\n",
        "    weights = model.fc.weight.detach().cpu().numpy()\n",
        "    embed_weights = model.embedding.weight.detach().cpu().numpy()\n",
        "\n",
        "    for i, label in enumerate(class_names):\n",
        "        sims = embed_weights @ weights[i]\n",
        "        top_indices = sims.argsort()[-top_n:][::-1]\n",
        "        top_words = [id2word.get(idx, '?') for idx in top_indices]\n",
        "        print(f\"\\nTop {top_n} indicative words for '{label}':\\n  {' '.join(top_words)}\")\n",
        "\n",
        "# LIME INTERPRETABILITY (POST-HOC)\n",
        "class WrappedModel:\n",
        "    def __init__(self, model, label_map):\n",
        "        self.model = model\n",
        "        self.label_map = label_map\n",
        "        self.model.eval()\n",
        "\n",
        "    def predict_proba(self, texts):\n",
        "        outputs = []\n",
        "        for text in texts:\n",
        "            tokens = preprocess_fn(text).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                logits = self.model(tokens)\n",
        "                probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "            outputs.append(probs)\n",
        "        return np.stack(outputs)\n",
        "\n",
        "def explain_with_lime(text, model, label_names):\n",
        "    explainer = LimeTextExplainer(class_names=label_names)\n",
        "    wrapped = WrappedModel(model, label_names)\n",
        "    explanation = explainer.explain_instance(text, wrapped.predict_proba, num_features=6)\n",
        "    explanation.show_in_notebook()\n",
        "\n",
        "# Intrinsic: top words per emotion class\n",
        "print_top_words(model1, id2word, label_names, top_n=10)\n",
        "\n",
        "# Interactive classifier\n",
        "classify_interactively(model1, label_names)\n",
        "\n",
        "# Post-hoc: LIME explanation for a specific sentence\n",
        "example_text = \"I feel miserable and alone\"\n",
        "explain_with_lime(example_text, model1, label_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_UrkQjeendJ"
      },
      "source": [
        "# Second model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BoW + MLP Classifier\n",
        "\n",
        "##Model Overview\n",
        "\n",
        "\t•\tArchitecture: Bag-of-Words (BoW) vectorization → Fully connected MLP (2 layers with ReLU and dropout)\n",
        "\t•\tInput: Sparse BoW vectors with max 5000 features representing word counts\n",
        "\t•\tOutput: Emotion classes (same 6 labels as before)\n",
        "\n",
        "##Training Setup\n",
        "\n",
        "\t•\tOptimizer: Adam with learning rate = 1e-3\n",
        "\t•\tLoss: CrossEntropyLoss (for multi-class classification)\n",
        "\t•\tEpochs: 10\n",
        "\t•\tBatch Size: 32\n",
        "\n",
        "##Results\n",
        "\n",
        "\t•\tBest Validation Accuracy: Around 88.95% (early epochs, around epoch 5)\n",
        "\t•\tFinal Test Accuracy: About 84.3%\n",
        "  \n",
        "\t•\tTraining Trend:\n",
        "\t•\tTraining loss steadily decreases across epochs, showing good fitting\n",
        "\t•\tValidation loss starts to increase after about 10-15 epochs, indicating overfitting\n",
        "\t•\tValidation accuracy peaks early and then slightly declines or plateaus\n",
        "\t•\tOverfitting: Clear signs after early epochs, model learns training data well but generalizes less as epochs progress\n",
        "\n",
        "##Conclusion\n",
        "\n",
        "This BoW + MLP model is a simple but effective baseline for text emotion classification. It leverages word count features instead of learned embeddings, making it fast to train and interpret. However, it is prone to overfitting if trained for too long.\n",
        "\n"
      ],
      "metadata": {
        "id": "h4tQ7rmkdsYF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLo7ZvYNeqk6"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "# 1. Load data using pandas from HuggingFace\n",
        "splits = {\n",
        "    'train': 'split/train-00000-of-00001.parquet',\n",
        "    'validation': 'split/validation-00000-of-00001.parquet',\n",
        "    'test': 'split/test-00000-of-00001.parquet'\n",
        "}\n",
        "\n",
        "df = {\n",
        "    split: pd.read_parquet(f\"hf://datasets/dair-ai/emotion/{path}\")\n",
        "    for split, path in splits.items()\n",
        "}\n",
        "\n",
        "# 2. Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "df['train']['label_encoded'] = label_encoder.fit_transform(df['train']['label'])\n",
        "df['validation']['label_encoded'] = label_encoder.transform(df['validation']['label'])\n",
        "\n",
        "# 3. Vectorize using BoW\n",
        "vectorizer = CountVectorizer(max_features=5000)\n",
        "X_train = vectorizer.fit_transform(df['train']['text']).toarray()\n",
        "X_val = vectorizer.transform(df['validation']['text']).toarray()\n",
        "X_test = vectorizer.transform(df['test']['text']).toarray()\n",
        "\n",
        "y_train = df['train']['label_encoded'].values\n",
        "y_val = df['validation']['label_encoded'].values\n",
        "y_test = label_encoder.transform(df['test']['label'])\n",
        "\n",
        "# 4. Convert to PyTorch Dataset\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_dataset = EmotionDataset(X_train, y_train)\n",
        "val_dataset = EmotionDataset(X_val, y_val)\n",
        "test_dataset = EmotionDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "loaders = {\n",
        "    'train': train_loader,\n",
        "    'validation': val_loader,\n",
        "    'test': test_loader\n",
        "}\n",
        "\n",
        "# 5. BoW MLP Classifier\n",
        "class BoWEmotionClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_classes, dropout=0.0):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc2 = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        return self.fc2(x)\n",
        "\n",
        "# Initialize model\n",
        "model2 = BoWEmotionClassifier(input_dim=5000, hidden_dim=128, num_classes=len(label_encoder.classes_))\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, device='cpu'):\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    train_losses, val_losses, val_accuracies = [], [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                preds = torch.argmax(outputs, dim=1)\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "        val_losses.append(avg_val_loss)\n",
        "        val_accuracies.append(accuracy)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {accuracy:.4f}\")\n",
        "\n",
        "    # Test accuracy\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in loaders['test']:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "    test_accuracy = correct / total\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "    # Plot\n",
        "    epochs = range(1, len(train_losses) + 1)\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(epochs, train_losses, label='Train Loss')\n",
        "    plt.title('Train Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.plot(epochs, val_losses, label='Val Loss', color='orange')\n",
        "    plt.title('Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 3, 3)\n",
        "    plt.plot(epochs, val_accuracies, label='Val Acc', color='green')\n",
        "    plt.title('Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.grid(True)\n",
        "\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "    return train_losses, val_losses, val_accuracies\n",
        "\n",
        "\n",
        "train_losses, val_losses, val_accuracies = train_model(model2, train_loader, val_loader, num_epochs=10, lr=1e-3, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "best_val_acc = max(val_accuracies)\n",
        "best_epoch = val_accuracies.index(best_val_acc) + 1\n",
        "print(f\"\\n Best Validation Accuracy: {best_val_acc:.4f} (at epoch {best_epoch})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3y-wJ7hhJzP"
      },
      "source": [
        "## Second model experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hyperparameter Tuning on BoW + MLP\n",
        "\n",
        "**Goal**: Improve model performance by testing combinations of regularization, optimizers, dropout, and momentum.\n",
        "\n",
        "**Configs Tested:**\n",
        "\n",
        "\t•\tPooling: Average vs. Max\n",
        "\t•\tRegularization: None, L1, L2\n",
        "\t•\tOptimizers: SGD (with/without momentum), Adam, AdamW\n",
        "\t•\tDropout: 0.0, 0.3\n",
        "\n",
        "Each config was trained for 10 epochs, with optional L1/L2 regularization applied. For each setup, we recorded:\n",
        "\n",
        "\t•\tTraining loss\n",
        "\t•\tValidation loss\n",
        "\t•\tValidation accuracy\n",
        "\t•\tTest accuracy\n",
        "\n",
        "Visualization: For every config, we plotted the training dynamics to identify trends and overfitting.\n"
      ],
      "metadata": {
        "id": "EcCp9vFOekDU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second model's interpretability"
      ],
      "metadata": {
        "id": "XuJFnb0fmMPz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interpretability\n",
        "import torch\n",
        "import numpy as np\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "# PREPROCESS FUNCTION FOR LIME & INTERACTIVE\n",
        "def preprocess_fn(text):\n",
        "    vector = vectorizer.transform([text]).toarray()[0]\n",
        "    return torch.tensor(vector, dtype=torch.float32)\n",
        "\n",
        "# INTERACTIVE CLASSIFICATION\n",
        "def classify_interactively(model, label_map):\n",
        "    print(\"\\nType a sentence to classify its emotion. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        text = input(\"Enter text: \")\n",
        "        if text.lower() == 'exit':\n",
        "            break\n",
        "        tokens = preprocess_fn(text).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = model(tokens)\n",
        "            probs = torch.softmax(logits, dim=1).squeeze()\n",
        "            pred = torch.argmax(probs).item()\n",
        "        print(f\"Prediction: {label_map[pred]} ({probs[pred]:.2f})\")\n",
        "        print(\"Top class probabilities:\")\n",
        "        for i, p in enumerate(probs):\n",
        "            print(f\"  {label_map[i]}: {p:.2f}\")\n",
        "\n",
        "# CLASS-WORD ASSOCIATION (INTRINSIC)\n",
        "def print_top_words(model, vectorizer, class_names, top_n=10):\n",
        "    weights = model.fc2.weight.detach().cpu().numpy()\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    input_weights = model.fc1.weight.detach().cpu().numpy()\n",
        "    combined_weights = weights @ input_weights\n",
        "\n",
        "    for i, class_name in enumerate(class_names):\n",
        "        top_indices = combined_weights[i].argsort()[-top_n:][::-1]\n",
        "        top_words = [feature_names[idx] for idx in top_indices]\n",
        "        print(f\"\\nTop {top_n} indicative words for '{class_name}':\")\n",
        "        print(\"  \" + \", \".join(top_words))\n",
        "\n",
        "\n",
        "# LIME INTERPRETABILITY (POST-HOC)\n",
        "class WrappedModel:\n",
        "    def __init__(self, model, label_map):\n",
        "        self.model = model\n",
        "        self.label_map = label_map\n",
        "        self.model.eval()\n",
        "\n",
        "    def predict_proba(self, texts):\n",
        "        outputs = []\n",
        "        for text in texts:\n",
        "            tokens = preprocess_fn(text).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                logits = self.model(tokens)\n",
        "                probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "            outputs.append(probs)\n",
        "        return np.stack(outputs)\n",
        "\n",
        "def explain_with_lime(text, model, label_names):\n",
        "    explainer = LimeTextExplainer(class_names=label_names)\n",
        "    wrapped = WrappedModel(model, label_names)\n",
        "    explanation = explainer.explain_instance(text, wrapped.predict_proba, num_features=6)\n",
        "    explanation.show_in_notebook()\n",
        "\n",
        "# Intrinsic: top words per emotion class\n",
        "class_names = label_encoder.classes_\n",
        "print_top_words(model2, vectorizer, class_names, top_n=10)\n",
        "\n",
        "# Interactive classifier\n",
        "classify_interactively(model2, label_names)\n",
        "\n",
        "# Post-hoc: LIME explanation for a specific sentence\n",
        "example_text = \"I feel miserable and alone\"\n",
        "explain_with_lime(example_text, model2, label_names)"
      ],
      "metadata": {
        "id": "-7gp7zFDmY7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Third model: transfomer"
      ],
      "metadata": {
        "id": "co7PQzYYpCS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader # Import DataLoader and TensorDataset here\n",
        "\n",
        "splits = {'train': 'split/train-00000-of-00001.parquet', 'validation': 'split/validation-00000-of-00001.parquet', 'test': 'split/test-00000-of-00001.parquet'}\n",
        "dataset = {\n",
        "    split: pd.read_parquet(f\"hf://datasets/dair-ai/emotion/{path}\")\n",
        "    for split, path in splits.items()\n",
        "}\n",
        "label_names = sorted(dataset[\"train\"][\"label\"].unique())\n",
        "label_names = [str(lbl) for lbl in label_names]  # In case labels are ints\n",
        "\n",
        "def label_stats(split):\n",
        "    # Ensure dataset[split] is a DataFrame here\n",
        "    if not isinstance(dataset[split], pd.DataFrame):\n",
        "        print(f\"Warning: dataset['{split}'] is not a DataFrame when calling label_stats.\")\n",
        "        # You might need to handle this case if label_stats is called after tensor conversion\n",
        "        # For this fix, we assume label_stats is called before the conversion.\n",
        "        return [], Counter() # Return empty results if not a DataFrame\n",
        "\n",
        "    labels = dataset[split][\"label\"].tolist()\n",
        "    counts = Counter(labels)\n",
        "    print(f\"\\n[{split.upper()}] Label Counts:\")\n",
        "    for label, count in sorted(counts.items()):\n",
        "        # Ensure label is within the bounds of label_names\n",
        "        if label < len(label_names):\n",
        "            print(f\"{label_names[label]:>10}: {count}\")\n",
        "        else:\n",
        "            print(f\"Unknown label ({label}): {count}\")\n",
        "    return labels, counts\n",
        "\n",
        "# Re-define splits as a list after the dictionary definition\n",
        "splits = ['train', 'validation', 'test']\n",
        "split_labels = {}\n",
        "for split in splits:\n",
        "    split_labels[split], counts = label_stats(split)\n",
        "\n",
        "# Ensure index is within bounds\n",
        "if most_common_class[0] < len(label_names):\n",
        "    print(f\"Majority class baseline accuracy: {majority_accuracy:.2f} ({label_names[most_common_class[0]]})\")\n",
        "else:\n",
        "     print(f\"Majority class baseline accuracy: {majority_accuracy:.2f} (Unknown label)\")\n",
        "\n",
        "\n",
        "# STEP 2: Tokenization\n",
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "# Apply tokenization while dataset is still a DataFrame\n",
        "for split in splits:\n",
        "    # Ensure dataset[split] is a DataFrame before applying .apply()\n",
        "    if isinstance(dataset[split], pd.DataFrame):\n",
        "        dataset[split][\"tokens\"] = dataset[split][\"text\"].apply(tokenize)\n",
        "    else:\n",
        "        print(f\"Warning: dataset['{split}'] is not a DataFrame during tokenization.\")\n",
        "\n",
        "# Text length stats\n",
        "# Calculate lengths while dataset[split] is still a DataFrame with a 'tokens' column\n",
        "lengths = {split: [len(ex['tokens']) for _, ex in dataset[split].iterrows()] for split in splits if isinstance(dataset[split], pd.DataFrame)} # Iterate over rows if it's a DataFrame\n",
        "for split in splits:\n",
        "    if split in lengths: # Check if length data was collected for this split\n",
        "        lens = lengths[split]\n",
        "        print(f\"\\n{split.capitalize()} text length - Min: {min(lens)}, Max: {max(lens)}, Mean: {np.mean(lens):.2f}, Std: {np.std(lens):.2f}\")\n",
        "    else:\n",
        "        print(f\"\\nCould not calculate text length stats for {split.capitalize()} (not a DataFrame).\")\n",
        "\n",
        "\n",
        "# STEP 3: Build vocabulary\n",
        "PAD_TOKEN = \"<PAD>\"\n",
        "OOV_TOKEN = \"<OOV>\"\n",
        "\n",
        "# Flatten all tokens in train split\n",
        "# Ensure dataset['train'] is a DataFrame before accessing 'tokens'\n",
        "all_tokens = []\n",
        "if isinstance(dataset['train'], pd.DataFrame):\n",
        "    all_tokens = [token for example in dataset['train']['tokens'] for token in example]\n",
        "else:\n",
        "    print(\"\\nWarning: dataset['train'] is not a DataFrame. Cannot build vocabulary from tokens.\")\n",
        "\n",
        "vocab = {PAD_TOKEN: 0, OOV_TOKEN: 1}\n",
        "if all_tokens: # Only build vocab if tokens were collected\n",
        "    for token in set(all_tokens):\n",
        "        vocab[token] = len(vocab)\n",
        "\n",
        "# Reverse vocab\n",
        "id2word = {idx: word for word, idx in vocab.items()}\n",
        "\n",
        "print(f\"\\nVocabulary size: {len(vocab)}\")\n",
        "\n",
        "# STEP 4: Encode texts with OOV handling\n",
        "def encode_tokens(tokens, vocab):\n",
        "    return [vocab.get(token, vocab[OOV_TOKEN]) for token in tokens]\n",
        "\n",
        "for split in splits:\n",
        "    # Ensure dataset[split] is a DataFrame before applying .apply()\n",
        "    if isinstance(dataset[split], pd.DataFrame):\n",
        "        dataset[split][\"input_ids\"] = dataset[split][\"tokens\"].apply(lambda tokens: encode_tokens(tokens, vocab))\n",
        "    else:\n",
        "         print(f\"Warning: dataset['{split}'] is not a DataFrame during encoding.\")\n",
        "\n",
        "\n",
        "# STEP 5: Padding and tensor conversion\n",
        "# Use MAX_LEN calculated from the 'lengths' dictionary\n",
        "# Check if 'lengths' was successfully populated\n",
        "if 'train' in lengths:\n",
        "    MAX_LEN = int(np.percentile(lengths['train'], 95))\n",
        "else:\n",
        "    MAX_LEN = 50 # Fallback value if lengths could not be calculated\n",
        "    print(f\"Warning: Could not determine MAX_LEN from lengths. Using default: {MAX_LEN}\")\n",
        "\n",
        "\n",
        "def pad_sequence(seq, max_len, pad_val):\n",
        "    # Ensure seq is a list\n",
        "    if not isinstance(seq, list):\n",
        "        print(f\"Warning: pad_sequence received non-list input: {type(seq)}\")\n",
        "        return [] # Return empty list if input is not list\n",
        "    return seq[:max_len] + [pad_val] * max(0, max_len - len(seq))\n",
        "\n",
        "# This is where the dataset is converted from DataFrame to dictionary of tensors\n",
        "for split in splits:\n",
        "    inputs = []\n",
        "    labels = []\n",
        "    # Ensure dataset[split] is a DataFrame with necessary columns before iteration\n",
        "    if isinstance(dataset[split], pd.DataFrame) and 'input_ids' in dataset[split].columns and 'label' in dataset[split].columns:\n",
        "        for index, example in dataset[split].iterrows(): # Iterate over DataFrame rows\n",
        "            # Ensure 'input_ids' is a list before padding\n",
        "            input_ids_list = example['input_ids']\n",
        "            if not isinstance(input_ids_list, list):\n",
        "                 print(f\"Warning: input_ids for example in {split} is not a list: {type(input_ids_list)}\")\n",
        "                 input_ids_list = [] # Use empty list if not a list\n",
        "\n",
        "            padded = pad_sequence(input_ids_list, MAX_LEN, vocab.get(PAD_TOKEN, -1)) # Use .get() for PAD_TOKEN to be safe, fallback to -1\n",
        "            inputs.append(padded)\n",
        "            labels.append(example['label'])\n",
        "\n",
        "        inputs_tensor = torch.tensor(inputs)\n",
        "        labels_tensor = torch.tensor(labels)\n",
        "        dataset[split] = {'inputs': inputs_tensor, 'labels': labels_tensor}\n",
        "    else:\n",
        "        print(f\"Warning: dataset['{split}'] is not a DataFrame with 'input_ids' and 'label' columns. Skipping tensor conversion for this split.\")\n",
        "        # If conversion failed, ensure dataset[split] is not expected to be tensors later\n",
        "        # Depending on the rest of the pipeline, you might want to remove this split from 'loaders' later\n",
        "\n",
        "# Create DataLoaders\n",
        "batch_size = 32\n",
        "loaders = {}\n",
        "for split in splits:\n",
        "    # Check if dataset[split] has the expected structure (dictionary with 'inputs' and 'labels' tensors)\n",
        "    if isinstance(dataset[split], dict) and 'inputs' in dataset[split] and 'labels' in dataset[split] and isinstance(dataset[split]['inputs'], torch.Tensor) and isinstance(dataset[split]['labels'], torch.Tensor):\n",
        "        tensor_dataset = TensorDataset(dataset[split]['inputs'], dataset[split]['labels'])\n",
        "        shuffle = True if split == 'train' else False\n",
        "        loaders[split] = DataLoader(tensor_dataset, batch_size=batch_size, shuffle=shuffle)\n",
        "    else:\n",
        "        print(f\"Warning: Could not create DataLoader for {split}. Dataset structure not as expected.\")\n",
        "\n",
        "\n",
        "print(\"\\n Data preparation complete. Loaders are ready!\")"
      ],
      "metadata": {
        "id": "yJOLE0gZpiDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Transformer-Based Classifier\n",
        "\n",
        "We trained a Transformer-based neural network for classifying emotions in tweets using the Emotion dataset.\n",
        "\n",
        "**Model Overview**\n",
        "\n",
        "\t•\tArchitecture: Embedding + Positional Encoding → Transformer Encoder (2 layers, 4 heads) → Mean Pooling → Linear classifier\n",
        "\t•\tType: Deep Transformer encoder-based model for sequence classification\n",
        "\n",
        "**Training Setup**\n",
        "\n",
        "\t•\tOptimizer: AdamW, LR = 2e-4\n",
        "\t•\tLoss: CrossEntropy\n",
        "\t•\tEpochs: 10\n",
        "\t•\tDevice: CPU/GPU (auto-selected)\n",
        "\n",
        "**Results**\n",
        "\n",
        "\t•\tInitial Val Accuracy: 53.3% (Epoch 0)\n",
        "\t•\tPeak Val Accuracy: ~86.65% (around Epochs 6-7)\n",
        "\t•\tFinal Test Accuracy: 86.20%\n",
        "\t•\tTraining Loss: Decreased steadily from ~1.5 to ~0.12 in first 10 epochs\n",
        "\t•\tValidation Loss: Initially drops, then stabilizes around 0.38-0.46 after epoch 4-5\n",
        "\t•\tTrend: Model learns quickly in early epochs, then stabilizes with minor fluctuations in validation metrics\n",
        "\t•\tOverall Accuracy: 86.20%\n",
        "\t•\tMacro F1-score: ~0.81\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "This Transformer model effectively classifies emotions with solid accuracy and balanced precision/recall across labels.\n",
        "\n",
        "The deep self-attention layers enable nuanced understanding of tweet semantics compared to simpler baselines.\n",
        "\n",
        "This model provides a strong foundation for further improvements like deeper layers, fine-tuning, or augmented data.\n"
      ],
      "metadata": {
        "id": "2s-8G8o6m0GZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import numpy as np\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_classes, num_layers, dropout=0.1, max_len=128):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(1, max_len, embed_dim))\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        x = self.embedding(x) + self.pos_embedding[:, :seq_len]\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.mean(dim=1)  # Mean pooling\n",
        "        return self.classifier(x)\n",
        "\n",
        "# Instantiate model\n",
        "model = TransformerClassifier(\n",
        "    vocab_size=len(vocab),\n",
        "    embed_dim=128,\n",
        "    num_heads=4,\n",
        "    hidden_dim=256,\n",
        "    num_classes=6,\n",
        "    num_layers=2,\n",
        "    max_len=128\n",
        ").to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)\n",
        "\n",
        "# Loaders\n",
        "train_loader = loaders['train']\n",
        "val_loader = loaders['validation']\n",
        "test_loader = loaders['test']\n",
        "\n",
        "# For plotting\n",
        "train_losses, val_accuracies = [], []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for input_ids, labels in train_loader:\n",
        "        input_ids, labels = input_ids.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(input_ids)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_loader)\n",
        "    train_losses.append(avg_train_loss)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    correct, total = 0, 0\n",
        "    val_loss = 0\n",
        "    all_preds, all_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids, labels in val_loader:\n",
        "            input_ids, labels = input_ids.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(input_ids)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    val_accuracy = correct / total * 100\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f\"epoch {epoch}: train loss: {avg_train_loss:.4f}, val loss: {avg_val_loss:.4f}, val acc: {val_accuracy:.2f}%\")\n",
        "\n",
        "\n",
        "# Final test accuracy\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "  for inputs, labels in loaders['test']:\n",
        "    inputs, labels = inputs.to(device), labels.to(device)\n",
        "    outputs = model(inputs)\n",
        "    preds = torch.argmax(outputs, dim=1)\n",
        "    correct += (preds == labels).sum().item()\n",
        "    total += labels.size(0)\n",
        "test_accuracy = correct / total\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "\n",
        "# Plot loss and accuracy\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy (%)')\n",
        "plt.title('Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Evaluation on Test Set\n",
        "model.eval()\n",
        "all_preds, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for input_ids, labels in test_loader:\n",
        "        input_ids, labels = input_ids.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(input_ids)\n",
        "        preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "        all_preds.extend(preds.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(all_labels, all_preds, digits=4))\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_preds)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot(cmap=\"Blues\", xticks_rotation='vertical')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mIV5JzAwpEHH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Third model's interpretability"
      ],
      "metadata": {
        "id": "yFxUoXxnrtmu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interpretability\n",
        "import numpy as np\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from captum.attr import IntegratedGradients\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Helper for Raw Text Prediction\n",
        "def preprocess_fn_transformer(text, max_len=128):\n",
        "    tokens = tokenize(text.lower())\n",
        "    token_ids = [vocab.get(token, vocab[OOV_TOKEN]) for token in tokens]\n",
        "    token_ids = token_ids[:max_len] + [vocab[PAD_TOKEN]] * max(0, max_len - len(token_ids))\n",
        "    return torch.tensor(token_ids, dtype=torch.long)\n",
        "\n",
        "# Interactive Classification\n",
        "def classify_interactively(model, label_map):\n",
        "    print(\"\\nType a sentence to classify its emotion. Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        text = input(\"Enter text: \")\n",
        "        if text.lower() == 'exit':\n",
        "            break\n",
        "        input_tensor = preprocess_fn_transformer(text).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_tensor)\n",
        "            probs = torch.softmax(logits, dim=1).squeeze()\n",
        "            pred = torch.argmax(probs).item()\n",
        "        print(f\"Prediction: {label_map[pred]} ({probs[pred]:.2f})\")\n",
        "        print(\"Top class probabilities:\")\n",
        "        for i, p in enumerate(probs):\n",
        "            print(f\"  {label_map[i]}: {p:.2f}\")\n",
        "\n",
        "#  LIME Wrapper\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "import numpy as np\n",
        "\n",
        "class WrappedTransformerModel:\n",
        "    def __init__(self, model):\n",
        "        self.model = model.eval()\n",
        "\n",
        "    def predict_proba(self, texts):\n",
        "        results = []\n",
        "        for text in texts:\n",
        "            input_tensor = preprocess_fn_transformer(text).unsqueeze(0).to(device)\n",
        "            with torch.no_grad():\n",
        "                logits = self.model(input_tensor)\n",
        "                probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "            results.append(probs)\n",
        "        return np.stack(results)\n",
        "\n",
        "def explain_with_lime(text, model, label_names):\n",
        "    explainer = LimeTextExplainer(class_names=label_names)\n",
        "    wrapped = WrappedTransformerModel(model)\n",
        "    explanation = explainer.explain_instance(text, wrapped.predict_proba, num_features=8)\n",
        "    explanation.show_in_notebook()\n",
        "\n",
        "\n",
        "# Integrated Gradients\n",
        "from captum.attr import IntegratedGradients\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def explain_with_integrated_gradients(text, model, label_idx, label_names):\n",
        "    model.eval()\n",
        "\n",
        "    # Tokenize input text and get input IDs (LongTensor)\n",
        "    input_ids = preprocess_fn_transformer(text).unsqueeze(0).to(device)\n",
        "\n",
        "    # embeddings\n",
        "    embeddings = model.embedding(input_ids) + model.pos_embedding[:, :input_ids.size(1)]\n",
        "    embeddings.requires_grad_()\n",
        "\n",
        "    def forward_embedded(x_emb):\n",
        "        x = model.transformer_encoder(x_emb)\n",
        "        x = x.mean(dim=1)  # mean pooling\n",
        "        return model.classifier(x)\n",
        "\n",
        "    ig = IntegratedGradients(forward_embedded)\n",
        "    baseline = torch.zeros_like(embeddings).to(device)\n",
        "\n",
        "    attributions = ig.attribute(\n",
        "        inputs=embeddings,\n",
        "        baselines=baseline,\n",
        "        target=label_idx,\n",
        "        return_convergence_delta=False,\n",
        "        internal_batch_size=1,\n",
        "        n_steps=50\n",
        "    )\n",
        "\n",
        "    token_ids = input_ids[0].cpu().tolist()\n",
        "    tokens = [id2word.get(idx, '<unk>') for idx in token_ids]\n",
        "    scores = attributions.squeeze(0).sum(dim=1).cpu().detach().numpy()  # attribution per token\n",
        "\n",
        "    plt.figure(figsize=(12, 2))\n",
        "    sns.barplot(x=tokens, y=scores, palette=\"coolwarm\")\n",
        "    plt.title(f\"Integrated Gradients Attribution for '{label_names[label_idx]}'\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 1. Interactive use\n",
        "classify_interactively(model, label_names)\n",
        "\n",
        "# 2. LIME\n",
        "explain_with_lime(\"I feel really frustrated and tired\", model, label_names)\n",
        "\n",
        "# 3. Integrated Gradients\n",
        "text = \"I feel really frustrated and tired\"\n",
        "input_ids = preprocess_fn_transformer(text).unsqueeze(0).to(device)\n",
        "with torch.no_grad():\n",
        "    pred_label = torch.argmax(model(input_ids), dim=1).item()\n",
        "explain_with_integrated_gradients(text, model, label_idx=pred_label, label_names=label_names)\n"
      ],
      "metadata": {
        "id": "jwda5GFerxXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-trained language model"
      ],
      "metadata": {
        "id": "O0uMmDxLupr_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We fine-tuned a pretrained BERT-base-uncased model for classifying emotions in tweets using the Emotion dataset.\n",
        "\n",
        "**Model** **Overview**\n",
        "\n",
        "\t•\tArchitecture: Pretrained BERT with a classification head (BertForSequenceClassification)\n",
        "\t•\tType: Transformer-based deep model with contextual embeddings\n",
        "\n",
        "**Training** Setup\n",
        "\n",
        "\t•\tOptimizer: AdamW, LR = 2e-5\n",
        "\t•\tLoss: CrossEntropy\n",
        "\t•\tEpochs: 3\n",
        "\t•\tBatch Size: 16 (train), 64 (eval)\n",
        "\t•\tDevice: GPU (if available)\n",
        "\n",
        "**Results**\n",
        "\n",
        "\t•\tBest Val Accuracy: 93.8% (Epoch 3)\n",
        "\t•\tFinal Test Accuracy: 92.9%\n",
        "\t•\tTrend: Steady decrease in training and validation loss, no overfitting observed\n",
        "\n",
        "**Conclusion**\n",
        "\n",
        "The pretrained BERT model performs strongly on emotion classification, achieving high accuracy and balanced precision/recall across classes. Class imbalance affects minority classes (e.g., label 5) but overall it’s a robust and reliable baseline outperforming simpler models.\n"
      ],
      "metadata": {
        "id": "0oPyfbX0otQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, TrainingArguments, Trainer\n",
        "import torch\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# 1) Load parquet data into pandas DataFrames\n",
        "splits = {\n",
        "    'train': 'split/train-00000-of-00001.parquet',\n",
        "    'validation': 'split/validation-00000-of-00001.parquet',\n",
        "    'test': 'split/test-00000-of-00001.parquet'\n",
        "}\n",
        "\n",
        "dataset = {\n",
        "    split: pd.read_parquet(f\"hf://datasets/dair-ai/emotion/{path}\")\n",
        "    for split, path in splits.items()\n",
        "}\n",
        "\n",
        "for split in splits:\n",
        "    counts = Counter(dataset[split]['label'])\n",
        "    print(f\"\\n[{split.upper()}] Label counts:\")\n",
        "    for label, count in sorted(counts.items()):\n",
        "        print(f\"  Label {label}: {count}\")\n",
        "\n",
        "# 2) Convert pandas DataFrames to HuggingFace Dataset objects\n",
        "def pandas_to_hf_dataset(df):\n",
        "    if 'text' not in df.columns:\n",
        "        raise ValueError(\"DataFrame must have a 'text' column with raw text!\")\n",
        "    return Dataset.from_pandas(df)\n",
        "\n",
        "hf_datasets = DatasetDict({\n",
        "    split: pandas_to_hf_dataset(dataset[split])\n",
        "    for split in splits\n",
        "})\n",
        "\n",
        "print(f\"\\nSample example from train split:\\n{hf_datasets['train'][0]}\")\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], truncation=True, padding='max_length', max_length=128)\n",
        "\n",
        "hf_datasets = hf_datasets.map(tokenize_function, batched=True)\n",
        "\n",
        "print(hf_datasets['train'].features)\n",
        "\n",
        "import torch\n",
        "\n",
        "def format_for_torch(example):\n",
        "    return {\n",
        "        \"input_ids\": torch.tensor(example[\"input_ids\"], dtype=torch.long),\n",
        "        \"attention_mask\": torch.tensor(example[\"attention_mask\"], dtype=torch.long),\n",
        "        \"label\": torch.tensor(example[\"label\"], dtype=torch.long),\n",
        "    }\n",
        "\n",
        "hf_datasets['train'] = hf_datasets['train'].map(format_for_torch)\n",
        "hf_datasets['validation'] = hf_datasets['validation'].map(format_for_torch)\n",
        "hf_datasets['test'] = hf_datasets['test'].map(format_for_torch)\n",
        "\n",
        "label_list = sorted(dataset['train']['label'].unique())\n",
        "num_labels = len(label_list)\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=num_labels)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=64,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    load_best_model_at_end=True,\n",
        "    report_to=[]\n",
        ")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    acc = np.mean(preds == labels)\n",
        "    return {\"accuracy\": acc}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=hf_datasets['train'],\n",
        "    eval_dataset=hf_datasets['validation'],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n",
        "\n",
        "# Test evaluation\n",
        "test_results = trainer.evaluate(hf_datasets['test'])\n",
        "print(f\"\\nTest accuracy: {test_results['eval_accuracy']:.4f}\")\n"
      ],
      "metadata": {
        "id": "GoFvZ0YKutJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train and validation plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "logs = trainer.state.log_history\n",
        "train_loss = [x['loss'] for x in logs if 'loss' in x and 'epoch' in x]\n",
        "val_loss = [x['eval_loss'] for x in logs if 'eval_loss' in x]\n",
        "val_acc = [x['eval_accuracy'] for x in logs if 'eval_accuracy' in x]\n",
        "epochs = [x['epoch'] for x in logs if 'eval_loss' in x]\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_loss[:len(epochs)], label=\"Train Loss\")\n",
        "plt.plot(epochs, val_loss, label=\"Val Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Train vs Validation Loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, val_acc, label=\"Val Accuracy\", color='green')\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Validation Accuracy\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "\n",
        "# Predict on test set\n",
        "preds_output = trainer.predict(hf_datasets['test'])\n",
        "y_true = preds_output.label_ids\n",
        "y_pred = np.argmax(preds_output.predictions, axis=1)\n",
        "\n",
        "# Confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1, 2, 3, 4, 5])\n",
        "disp.plot(cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Per-class\n",
        "report = classification_report(y_true, y_pred, digits=4)\n",
        "print(\"Classification Report:\\n\", report)"
      ],
      "metadata": {
        "id": "UcEP36eAwZux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pre-trained model's interpretability"
      ],
      "metadata": {
        "id": "7mk76RFjxfjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Interpretability of BERT Emotion Classifier\n",
        "\n",
        "**Methods Used:**\n",
        "\n",
        "**LIME (Local Interpretable Model-agnostic Explanations):**\n",
        "\n",
        "Provides word-level explanations by highlighting which tokens most influenced the model’s prediction on a given input sentence.\n",
        "Example: For “I feel extremely anxious and overwhelmed.”, LIME highlights “anxious” and “overwhelmed” as key words driving the emotion prediction.\n",
        "\n",
        "\n",
        "**Integrated Gradients (IG):**\n",
        "\n",
        "An intrinsic attribution method applied to BERT embeddings to assign importance scores to each input token relative to the predicted emotion class.\n",
        "Visualized as a bar plot over tokens, showing contribution strength. Words like “anxious” and “overwhelmed” receive the highest attribution scores.\n",
        "\n",
        "**Summary:**\n",
        "\n",
        "Both LIME and IG confirm the model relies heavily on emotionally charged words to make decisions, helping us peek inside the black box and understand why the classifier outputs a particular emotion."
      ],
      "metadata": {
        "id": "Q_fbx2fip1BQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interpretability\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import BertTokenizer\n",
        "from captum.attr import IntegratedGradients, visualization\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "import numpy as np\n",
        "\n",
        "# LIME Interpretability\n",
        "class WrappedHFModel:\n",
        "    def __init__(self, model, tokenizer):\n",
        "        self.model = model.eval()\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def predict_proba(self, texts):\n",
        "        results = []\n",
        "        for text in texts:\n",
        "            enc = self.tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=128)\n",
        "            input_ids = enc['input_ids'].to(model.device)\n",
        "            attention_mask = enc['attention_mask'].to(model.device)\n",
        "            with torch.no_grad():\n",
        "                logits = self.model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "                probs = torch.softmax(logits, dim=1).cpu().numpy()[0]\n",
        "            results.append(probs)\n",
        "        return np.stack(results)\n",
        "\n",
        "def explain_with_lime_bert(text, model, tokenizer, label_names):\n",
        "    explainer = LimeTextExplainer(class_names=label_names)\n",
        "    wrapped = WrappedHFModel(model, tokenizer)\n",
        "    explanation = explainer.explain_instance(text, wrapped.predict_proba, num_features=10)\n",
        "    explanation.show_in_notebook()\n",
        "\n",
        "\n",
        "# Integrated Gradients Interepretability\n",
        "def explain_with_ig_bert(text, model, tokenizer, label_names, label_idx=None):\n",
        "    model.eval()\n",
        "    encoded = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=128).to(model.device)\n",
        "    input_ids = encoded['input_ids']\n",
        "    attention_mask = encoded['attention_mask']\n",
        "\n",
        "    # Get embeddings for input_ids (this is the baseline input for IG)\n",
        "    embeddings = model.bert.embeddings.word_embeddings(input_ids)\n",
        "\n",
        "    def forward_func(inputs_embeds):\n",
        "        outputs = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
        "        return outputs.logits\n",
        "\n",
        "    # Get predicted label if none provided\n",
        "    if label_idx is None:\n",
        "        with torch.no_grad():\n",
        "            logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "            label_idx = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "    ig = IntegratedGradients(forward_func)\n",
        "    baseline = torch.zeros_like(embeddings).to(model.device)\n",
        "\n",
        "    attributions, delta = ig.attribute(\n",
        "        inputs=embeddings,\n",
        "        baselines=baseline,\n",
        "        target=label_idx,\n",
        "        return_convergence_delta=True\n",
        "    )\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
        "    scores = attributions.sum(dim=2).squeeze(0).cpu().detach().numpy()\n",
        "\n",
        "    # Plot attributions\n",
        "    plt.figure(figsize=(12, 2))\n",
        "    sns.barplot(x=tokens, y=scores, palette='coolwarm')\n",
        "    plt.title(f\"Integrated Gradients Attribution for class '{label_names[label_idx]}'\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "text = \"I feel extremely anxious and overwhelmed.\"\n",
        "explain_with_lime_bert(text, model, tokenizer, label_list)\n",
        "explain_with_ig_bert(text, model, tokenizer, label_list)"
      ],
      "metadata": {
        "id": "oFNRGAxKxkmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## pre-trained model experiment"
      ],
      "metadata": {
        "id": "mjDbyoWgwbDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import os\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "learning_rates = [2e-5, 3e-5, 5e-5]\n",
        "weight_decays = [0.0, 0.01, 0.1]\n",
        "batch_sizes = [16, 32]\n",
        "num_epochs = [3, 5]\n",
        "\n",
        "experiment_results = []\n",
        "\n",
        "for lr, wd, bs, epochs in itertools.product(learning_rates, weight_decays, batch_sizes, num_epochs):\n",
        "    exp_name = f\"lr{lr}_wd{wd}_bs{bs}_ep{epochs}\"\n",
        "    print(f\"\\n Running experiment: {exp_name}\")\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=f\"./results/{exp_name}\",\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"no\",\n",
        "        learning_rate=lr,\n",
        "        per_device_train_batch_size=bs,\n",
        "        per_device_eval_batch_size=64,\n",
        "        num_train_epochs=epochs,\n",
        "        weight_decay=wd,\n",
        "        logging_dir=f\"./logs/{exp_name}\",\n",
        "        load_best_model_at_end=False,\n",
        "        report_to=[],\n",
        "    )\n",
        "\n",
        "    model = BertForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-uncased\", num_labels=num_labels\n",
        "    )\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=hf_datasets['train'],\n",
        "        eval_dataset=hf_datasets['validation'],\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=lambda p: {\"accuracy\": np.mean(np.argmax(p.predictions, axis=1) == p.label_ids)},\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    test_result = trainer.evaluate(hf_datasets['test'])\n",
        "    y_pred = np.argmax(trainer.predict(hf_datasets['test']).predictions, axis=1)\n",
        "    y_true = hf_datasets['test']['label']\n",
        "\n",
        "    experiment_results.append({\n",
        "        \"experiment\": exp_name,\n",
        "        \"test_accuracy\": test_result[\"eval_accuracy\"],\n",
        "        \"confusion_matrix\": confusion_matrix(y_true, y_pred),\n",
        "        \"classification_report\": classification_report(y_true, y_pred, output_dict=True)\n",
        "    })\n",
        "\n",
        "    print(f\"{exp_name} | Test Acc: {test_result['eval_accuracy']:.4f}\")"
      ],
      "metadata": {
        "id": "MQn-uQ83we14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning on BERT for Emotion Classification\n",
        "\n",
        "**Goal**: Find the best combination of learning rate, weight decay, batch size, and epochs to maximize accuracy.\n",
        "\n",
        "**Configs Tested:**\n",
        "\n",
        "\t•\tLearning Rates: 2e-5, 3e-5, 5e-5\n",
        "\t•\tWeight Decays: 0.0, 0.01, 0.1\n",
        "\t•\tBatch Sizes: 16, 32\n",
        "\t•\tEpochs: 3, 5\n",
        "\n",
        "For each config, we recorded:\n",
        "\n",
        "\t•\tTest accuracy\n",
        "\t•\tConfusion matrix\n",
        "\t•\tClassification report\n"
      ],
      "metadata": {
        "id": "1KDq01p3qLDs"
      }
    }
  ]
}